{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib.colors import LogNorm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.adamw import AdamW\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms.v2 import Compose, ToImage, ToDtype, Lambda\n",
    "from tqdm import tqdm\n",
    "\n",
    "from reference_models.heidenreich.vae import VAE\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM = Compose(\n",
    "    [\n",
    "        ToImage(),\n",
    "        ToDtype(torch.float32, scale=True),\n",
    "        Lambda(lambda x: x.view(-1) - 0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "DATA_PATH = \"/mnt/data/quinn\"\n",
    "TRAIN_DATA = MNIST(\n",
    "    DATA_PATH,\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=TRANSFORM,\n",
    ")\n",
    "TEST_DATA = MNIST(\n",
    "    DATA_PATH,\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=TRANSFORM,\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LEARN_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-2\n",
    "N_EPOCHS = 50\n",
    "INPUT_DIM = 784\n",
    "LATENT_DIM = 2\n",
    "HIDDEN_DIM = 512\n",
    "\n",
    "TRAIN_LOADER = DataLoader(\n",
    "    TRAIN_DATA, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True\n",
    ")\n",
    "TEST_LOADER = DataLoader(\n",
    "    TEST_DATA, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True\n",
    ")\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "MODEL = VAE(INPUT_DIM, HIDDEN_DIM, LATENT_DIM).to(DEVICE)\n",
    "OPT = AdamW(MODEL.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "WRITER = SummaryWriter(f'runs/mnist/vae_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: VAE,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: Optimizer,\n",
    "    prev_updates: int,\n",
    "    writer: SummaryWriter | None = None,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Trains the model on the given data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        loss_fn: The loss function.\n",
    "        optimizer: The optimizer.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    n_upd = prev_updates\n",
    "\n",
    "    def try_calculate_grad(loss, output) -> None:\n",
    "        if n_upd % 100 != 0:\n",
    "            return\n",
    "\n",
    "        total_norm = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1.0 / 2)\n",
    "\n",
    "        print(\n",
    "            f\"Step {n_upd:,} (N samples: {n_upd*BATCH_SIZE:,}), Loss: {loss.item():.4f} (Recon: {output.loss_recon.item():.4f}, KL: {output.loss_kl.item():.4f}) Grad: {total_norm:.4f}\"\n",
    "        )\n",
    "\n",
    "        if writer is not None:\n",
    "            global_step = n_upd\n",
    "            writer.add_scalar(\"Loss/Train\", loss.item(), global_step)\n",
    "            writer.add_scalar(\"Loss/Train/BCE\", output.loss_recon.item(), global_step)\n",
    "            writer.add_scalar(\"Loss/Train/KLD\", output.loss_kl.item(), global_step)\n",
    "            writer.add_scalar(\"GradNorm/Train\", total_norm, global_step)\n",
    "\n",
    "    def update(data: torch.Tensor) -> None:\n",
    "        optimizer.zero_grad()\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "\n",
    "        output = model(data)\n",
    "        loss = output.loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        try_calculate_grad(loss, output)\n",
    "\n",
    "        clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    for data, _ in tqdm(dataloader):\n",
    "        n_upd += 1\n",
    "        update(data.to(DEVICE))\n",
    "\n",
    "    return prev_updates + len(dataloader)\n",
    "\n",
    "\n",
    "def test(\n",
    "    model: VAE,\n",
    "    dataloader: DataLoader,\n",
    "    cur_step: int,\n",
    "    writer: SummaryWriter | None = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Tests the model on the given data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to test.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        cur_step (int): The current step.\n",
    "        writer: The TensorBoard writer.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_recon_loss = 0\n",
    "    test_kl_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, _ in tqdm(dataloader, desc=\"Testing\"):\n",
    "            data = data.to(DEVICE)\n",
    "            data = data.view(data.size(0), -1)  # Flatten the data\n",
    "\n",
    "            output = model(data, compute_loss=True)  # Forward pass\n",
    "\n",
    "            test_loss += output.loss.item()\n",
    "            test_recon_loss += output.loss_recon.item()\n",
    "            test_kl_loss += output.loss_kl.item()\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    test_recon_loss /= len(dataloader)\n",
    "    test_kl_loss /= len(dataloader)\n",
    "    print(\n",
    "        f\"====> Test set loss: {test_loss:.4f} (BCE: {test_recon_loss:.4f}, KLD: {test_kl_loss:.4f})\"\n",
    "    )\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_scalar(\"Loss/Test\", test_loss, global_step=cur_step)\n",
    "        writer.add_scalar(\n",
    "            \"Loss/Test/BCE\", output.loss_recon.item(), global_step=cur_step\n",
    "        )\n",
    "        writer.add_scalar(\"Loss/Test/KLD\", output.loss_kl.item(), global_step=cur_step)\n",
    "\n",
    "        # Log reconstructions\n",
    "        writer.add_images(\n",
    "            \"Test/Reconstructions\",\n",
    "            output.x_recon.view(-1, 1, 28, 28),\n",
    "            global_step=cur_step,\n",
    "        )\n",
    "        writer.add_images(\n",
    "            \"Test/Originals\", data.view(-1, 1, 28, 28), global_step=cur_step\n",
    "        )\n",
    "\n",
    "        # Log random samples from the latent space\n",
    "        z = torch.randn(16, LATENT_DIM).to(DEVICE)\n",
    "        samples = model.decode(z)\n",
    "        writer.add_images(\n",
    "            \"Test/Samples\", samples.view(-1, 1, 28, 28), global_step=cur_step\n",
    "        )\n",
    "\n",
    "\n",
    "def plot(model: VAE, train_loader: DataLoader) -> None:\n",
    "    z = torch.randn(64, LATENT_DIM).to(DEVICE)\n",
    "    samples = model.decode(z)\n",
    "    # samples = torch.sigmoid(samples)\n",
    "\n",
    "    # print first sample\n",
    "    # print(samples[0])\n",
    "\n",
    "    # Plot the generated images\n",
    "    fig, ax = plt.subplots(8, 8, figsize=(8, 8))\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            ax[i, j].imshow(\n",
    "                samples[i * 8 + j].view(28, 28).cpu().detach().numpy(), cmap=\"gray\"\n",
    "            )\n",
    "            ax[i, j].axis(\"off\")\n",
    "\n",
    "    # plt.show()\n",
    "    plt.savefig(\"vae_mnist.webp\")\n",
    "\n",
    "    # encode and plot the z values for the train set\n",
    "    model.eval()\n",
    "    z_all = []\n",
    "    y_all = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(train_loader, desc=\"Encoding\"):\n",
    "            data = data.to(DEVICE)\n",
    "            output = model(data, compute_loss=False)\n",
    "            z_all.append(output.z_sample.cpu().numpy())\n",
    "            y_all.append(target.numpy())\n",
    "\n",
    "    z_all = np.concatenate(z_all, axis=0)\n",
    "    y_all = np.concatenate(y_all, axis=0)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(z_all[:, 0], z_all[:, 1], c=y_all, cmap=\"tab10\")\n",
    "    plt.colorbar()\n",
    "    # plt.show()\n",
    "    plt.savefig(\"vae_mnist_2d_scatter.webp\")\n",
    "\n",
    "    # plot as 2d histogram, log scale\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.hist2d(z_all[:, 0], z_all[:, 1], bins=128, cmap=\"Blues\", norm=LogNorm())\n",
    "    plt.colorbar()\n",
    "    # plt.show()\n",
    "    plt.savefig(\"vae_mnist_2d_hist.webp\")\n",
    "\n",
    "    # plot 1d histograms\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].hist(z_all[:, 0], bins=100, color=\"b\", alpha=0.7)\n",
    "    ax[0].set_title(\"z1\")\n",
    "    ax[1].hist(z_all[:, 1], bins=100, color=\"b\", alpha=0.7)\n",
    "    ax[1].set_title(\"z2\")\n",
    "    # plt.show()\n",
    "    plt.savefig(\"vae_mnist_1d_hist.webp\")\n",
    "\n",
    "    n = 15\n",
    "    z1 = torch.linspace(-0, 1, n)\n",
    "    z2 = torch.zeros_like(z1) + 2\n",
    "    z = torch.stack([z1, z2], dim=-1).to(DEVICE)\n",
    "    samples = model.decode(z)\n",
    "    samples = torch.sigmoid(samples)\n",
    "\n",
    "    # Plot the generated images\n",
    "    fig, ax = plt.subplots(1, n, figsize=(n, 1))\n",
    "    for i in range(n):\n",
    "        ax[i].imshow(samples[i].view(28, 28).cpu().detach().numpy(), cmap=\"gray\")\n",
    "        ax[i].axis(\"off\")\n",
    "\n",
    "    plt.savefig(\"vae_mnist_interp.webp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_updates = 0\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "    prev_updates = train(MODEL, TRAIN_LOADER, OPT, prev_updates, writer=WRITER)\n",
    "    test(MODEL, TEST_LOADER, prev_updates, writer=WRITER)\n",
    "\n",
    "plot(MODEL, TRAIN_LOADER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malleus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
